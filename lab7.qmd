---
title: "Lab 7: Logistic Regression and Support Vector Machines"
author: "Kaori Hirano"
date: "07/16/20"
format: pdf
---

# Packages

```{r load-packages}
# load packages here
library(doParallel)
library(tree) # for simple decision trees
library(ggdendro) # for plotting decision trees using ggplot
suppressPackageStartupMessages(library(randomForest)) # for random forests
library(gbm) # for gradient boosting machines
library(BART) # for Bayesian Additive Regression Trees
suppressPackageStartupMessages(library(caret)) # for cross-validating GBM
suppressPackageStartupMessages(library(tidyverse))
library(patchwork) # for combining ggplots
suppressPackageStartupMessages(library(pROC))
suppressPackageStartupMessages(library(ROCR))
suppressPackageStartupMessages(library(caret))
```

# Data  

```{r load-data}
# load data here
load("data/vendor_data.RData")

```


# Exercises 

## Data Visualization

### Q1
Make a visualization that shows the relationship between our outcome recent_receipt_7
and one of the predictors that you are interested in. Use an appropriate visualization for the
comparison you are making. What does your visualization tell you?
```{r data-viz1}
# plot of paying the fee and the stall type
ggplot(vendor_data) +
  geom_bar(aes(x = recent_receipt_7, fill = stall_type), na.rm = TRUE) +
  theme_classic() +
  labs(title = 'Fee in last 7 days by days present in market', x = 'Days at Market', y = 'Fee Paid in last 7 days (0 = no, 1 = yes)')
```
This visualization compares whether or not stalls paid the fee based on the stall type. The chart shows us that there are more stalls that did not pay the fee than did pay the fee. Similarly, we can see that among those that did pay the fee, there seem to by more temporary stalls that get taken up/down everyday, uncovered permanent stalls, and covered permanent stalls without a lock than covered permanent stalls without a lock or a tarp/blanket/basket on the ground comparatively to the number that did not pay the fee. We are also able to see the distribution of which stall types are most populous, with many tarp/blanket/basket stalls. 


```{r data-viz2}
#| warning: false
# comparison of days at the market per week and profit

ggplot(vendor_data, aes(x = days_pr_week, y = hh_income_trim_99), na.rm = TRUE) +
  geom_point() + 
  geom_jitter() + 
  theme_classic() + 
  labs(title = 'Yearly Household Income by Number of Days in the Market', x = 'Days at Market', y = 'Household Income (kwacha)') 
```
This visualization shows us that there doesn't appear to be a notable effect of the number of days a vendor is open at the market and their yearly household income. There are a large number of higher income households from both the 1 day at the market and 7 days at the market, while there are fewer in the 2, 3 and 6 days, with very few higher income households in the 4 and 5 days. This is not what I expected to see as a relationship between household income and days open. 

```{r data-viz3}
#| warning: false
# comparison of days at the market per week and profit

ggplot(vendor_data, aes(x = yrs_in_mkt_fix, y = log(profit)), na.rm = TRUE) +
  geom_point() + 
  geom_jitter() + 
  theme_classic() + 
  labs(title = 'Log Daily Profit by Years in the Market', x = 'Years at Market', y = 'Log Dailiy Profit (kwacha)')+ 
  geom_smooth()
```
There does not appear to be a notable relationship between profit and years at the market. There are many new stalls (less than 10 years) at the market, and fewer older stalls at the market. Among those stalls, there does not appear to be a visible overall change in income as the stall age changes. 


## Train-Test Split
```{r data-wrangling}
# creates new df for edited data
d <- data.frame(vendor_data)

# drops all na for outcome variable
d <- d %>% drop_na(recent_receipt_7)

# changes recent receipt 7 to a factor and renames for spelling accuracy
d$recent_receipt_7 <- as.factor(d$recent_receipt_7)

# changes district into factor
d$district <- as.factor(d$district)

# test/train split
set.seed(20)

# 75 25 split
sample1 <- sample(c(TRUE, FALSE), nrow(d), replace=TRUE, prob=c(0.75,0.25))
train <- d[sample1, ]
test <- d[!sample1, ]
```
ANSWER THE WORD QUESTION


### Q3 
```{r tree }
# makes tree for train, no market or id 
tree_train <- tree(recent_receipt_7 ~ . -market, train)

# gets predictions
tree_pred <- predict(tree_train, test,
    type = "class")
# gets ideal cost-complexity (k) by error rate
set.seed(21)
cv_tree <- cv.tree(tree_train, FUN = prune.misclass, K = 5)
cv_tree
```

```{r plot-cv-results-ggplot2}
tree_data <- data.frame(size = cv_tree$size,
                        complexity = cv_tree$k,
                        errors = cv_tree$dev)

p1 <- ggplot(tree_data, aes(x = size, y = errors)) +
  geom_point() +
  geom_line() +
  labs(x = "Tree Size (Terminal Nodes)",
       y = "Cross-Validation Errors",
       title = "Panel A: Tree Size vs Errors") +
  theme_classic()

p2 <- ggplot(tree_data, aes(x = complexity, y = errors)) +
  geom_point() +
  geom_line() +
  labs(x = "Cost-Complexity Parameter",
       title = "Panel B: Cost vs Errors") +
  theme_classic()

p1 + p2

```

```{r plot-tree}
#| warning: false
# plot tree function from example code

# defining functions that takes a named list of variable levels and a 
# column of variable labels and returns properly formatted labels
format_tree_labels <- function(labels, levels) {
  sapply(labels, \(x) if(grepl(":", x)) clean_col(x, levels) else clean_lt(x))
} 

# replace letter positions with actual level labels
clean_col <- function(x, levels){
  # split the label into label and levels
  x <- str_split_1(x, ":")
  # make new temp objects for the two components
  var <- x[1]
  levs_ids <- x[2]
  # get levels for correct variable
  levs <- levels[[var]]
  # get level ids for *relevant* levels
  levs_ids <- str_split_1(levs_ids, "") 
  levs_ids <- sapply(levs_ids, \(x) which(letters == x))
  # cut down levs to only the required levels
  levs <- levs[levs_ids]
  # paste everything together and return (immplicitly)
  paste0(var, ": ", paste0(levs, collapse = ", "))
}

# space out labels that include only "<"
clean_lt <- function(x){
  # split on <, then recombine with spaces before and after <
  x <- str_split_1(x, "<")
  paste0(x, collapse = " < ")
}

plot_tree <- function(model){
  require(ggdendro)
  # extract necessary information from tree object so that it is ggplotable
  tree_data <- dendro_data(model)
  # create a data frame with the split *values* which dendro_data() doesn't extract
  frame <- model$frame %>%
    rownames_to_column(var = "split") %>%
    mutate(splits = as.data.frame(splits)) %>% 
    unnest(cols = c(splits)) %>% 
    filter(var != "<leaf>") %>% 
    select(cutleft)
  
  # add the splits information in, which dendro_data() misses
  tree_data$labels <- tree_data$labels %>% 
    bind_cols(frame) %>% 
    mutate(label = paste0(as.character(label), cutleft),
           label = format_tree_labels(label, attr(model, "xlevels")))
      
  ggplot(segment(tree_data)) +
    geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
    geom_text(data = label(tree_data), 
              aes(x = x, y = y, label = label), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 1.5, size = 2) +
    theme_dendro()
}

prune_tree <- prune.misclass(tree_train, best = 5)
tree_plot <- plot_tree(prune_tree)
tree_plot + labs(title = "Decision Tree for Market Vendor Data") +
  theme(plot.title = element_text(hjust = 0.5))
```
```{r roc-tree}
# gets predicted 
predicted_probs <- predict(tree_train, test, type = "vector")[,2]

#gets real values of y
true <- test$recent_receipt_7

#makes roc 
roc_obj <- roc(true, predicted_probs)

#plots roc
plot(roc_obj, main = "ROC Curve", print.auc = TRUE, auc.polygon = TRUE, grid = TRUE)

```
The AUC for the single classification tree is .690


Use cross-validation to find the optimal cost-complexity parameter for a single classification
tree using the tree package. Use 5 folds and set the seed to 21 beforehand.
Plot the optimal tree. What does it tell you about who had evidence of paying the tax or
not?
Then, plot the ROC curve and calculate the accuracy and the AUC for this classifier using the
test data

### Q4
Use the caret package’s cross-validation framework to find the optimal random forest by
searching over mtry = 10:16 with 5-fold cross-validation.4
Inside the train() function, also
add na.action = na.roughfix.
5 Set the seed to 22 before doing cross-validation.
```{r rf}
# sets seed
set.seed(22)

# sets training parameters
train_control <- trainControl(method="cv", number = 5)

# gets grid for mtry
tune_grid <- expand.grid(mtry = c(10,11,12,13,14,15,16))

# editing data to have only numerics because roughfix
# can only work with numerics
train_numeric <- train %>% select(female, age, married, educ_num, houses, acres_farmland, bicycles, chickens, goats, basic_cell_phones, smart_phones, days_pr_week, service, yrs_in_mkt_fix, profit, hh_income_trim_99, customers_pr_day_trim_99, vote_intend)

#rough fixes for numeric only 
train_numeric <- na.roughfix(train_numeric)

# does training
best_forest <- train(recent_receipt_7 ~ . -market, data = train, trControl = train_control, method="rf",
               tuneGrid = tune_grid,
               verbose = FALSE,
               na.action = na.roughfix)

```


Nicely plot the variable importances for the optimal fit on training data. Plot only the top
10 most influential variables. Which variables seem most important for predicting whether a
vendor would be able to present a receipt or not?


Then, plot the ROC curve and calculate the accuracy and the AUC for this classifier using the
test data.
```{r roc-rf}
# gets predicted 
predicted_probs <- predict.train(best_forest, test, type = "prob", na.action = na.pass)[,2]

#gets real values of y
true <- test$recent_receipt_7

length(true)
length(predicted_probs)
#makes roc 
roc_obj <- roc(true, predicted_probs)

#plots roc
plot(roc_obj, main = "ROC Curve", print.auc = TRUE, auc.polygon = TRUE, grid = TRUE)

```

# predict.train()
The way in which caret::train() passes data to the machine learning algorithm chosen
by the user can complicate prediction in some cases, especially when there are factor level
predictors. Therefore, instead of calling predict(trained_object$finalModel), which
will call the predict() method for whatever the class of the finalModel is (in this case
"randomForest"), you should use predict.train(). In other words, call predict()
directly on the output of the train() function.
Set type = "prob" for the ROC curve and the AUC; set it to "raw" when calculating
the accuracy. In both cases, add na.action = na.roughfix to the predict() function
call. This is an imperfect solution but prevents NA’s from being dropped.

## Full Models

### Q5 
Use the caret package’s cross-validation framework to do 5-fold cross-validation to find the
optimal GBM by searching over the parameter grid in the following code chunk. Set the seed
to 23 before doing so. Inside the train() function, also add na.action = na.pass.
6 Please
note that the grid-search cross-validation could take a considerable amount of
time. To help with this, the code chunk below makes it so that iterations are run in parallel
– at the same time. This can speed up computation considerabl

```{r gbm-grid}
#| warning: false

# set up parallel processing
gbm_clusters <- makeCluster(detectCores() - 1)
registerDoParallel(gbm_clusters)

# grid to search over
expand.grid(n.trees = c(3000, 5000), 
            interaction.depth = c(1, 2, 3, 4),
            shrinkage = 10^(-3:-1),
            n.minobsinnode = 10)


set.seed(23)
# define cross-validation
train_control <- trainControl(method="cv", number = 5)
# set up tuning grid
# note that this implies 72 * 10 = 720 model fits!
tuneGrid_boost <- expand.grid(n.trees = c(1000, 2000, 3000), 
                              interaction.depth = c(1, 2, 3, 4),
                              shrinkage = 10^(-4:0),
                              n.minobsinnode = 10)

best_gbm <- train(recent_receipt_7 ~ . -market, data = train,
               trControl = train_control, method="gbm",
               tuneGrid = tuneGrid_boost,
               verbose = FALSE,
               na.action = na.pass)
# summarize results
print(best_gbm)

# prints best results
best_gbm$bestTune

# stop cluster when done; this is very important!
stopCluster(gbm_clusters)
```
Nicely plot the partial dependence plot between the outcome and customers_pr_day_trim_99.
What does it tell you about the relationship between the number of customers a vendor reports
having each day and the probability that the vendor presented a receipt?
```{r partial-d-plot}
# getting data frame
best_gbm_age_partdep <- plot(best_gbm$finalModel, i = "customers_pr_day_trim_99",  return.grid = TRUE)
# plotting data frame
best_gbm_age_partdep %>% 
  ggplot(aes(x = customers_pr_day_trim_99, y = y)) +
  geom_line() +
  labs(x = "Customers Per Day",
       y = "Paying Fee in Past 7 Days",
       title = "Partial Dependence Plot Of Paying Fee in Past 7 Days by Customers Per Day") +
  theme_bw()
```

This plot illustrates that the marginal effect of the number of customers per day is decreasing the likelihood of presenting a receipt after integrating the effects of other variables. 

Then, plot the ROC curve and calculate the accuracy and the AUC for this classifier using the
test data
```{r roc-gbm-not-done} 
# gets predicted 
predicted_probs <- predict(best_gbm, test, type = "prob", na.action = na.pass)[,2]

#gets real values of y
true <- test$recent_receipt_7

length(true)
length(predicted_probs)
#makes roc 
roc_obj <- roc(true, predicted_probs)

#plots roc
plot(roc_obj, main = "ROC Curve", print.auc = TRUE, auc.polygon = TRUE, grid = TRUE)
```

The AUC for this classifier using the test data is .716. 

### Q6
####1 
Is the peformance of any of any of the modeling approaches good, do you think? What might
this mean?
None of these methods are great at predicting whether or not a vendor has paid the fee. Based on the AUCs of around .6, they are all barely better than chance (which is .5). This likely means that the data needs a different type of modelling to be better understood. 

Which modeling approach resulted in the best fit? What were the optimal tuning parameters of
the best-fitting model? What does this tell you about the relationship between the predictors
and the outcome?

The modelling approach that resulted in the best fit is the GBM, which had an AUC of .716. The optimal tuning parameters of this model were 3000 trees, an interaction depth of 4, shrinkage of .001, and a number of minimum in nodes is 10. This tells us that there are many complex relationships between predictor variables because of the interaction depth, a slow learning process was needed for fitting the optimal model, and the min number of observations in terminal nodes means that we prioritized avoiding overfitting in this model. 

###Q7
In this homework, we have been working to predict when a Malawian market vendor will pay
the market tax or not. From the perspective of the government, this would be a great tool,
because they could potentially identify shirkers ahead of time. But what do you think could
be some of the downsides of this prediction task? In your opinion, is prediction of this kind
mostly good idea

Yes, there are downsides to this if used by the government. Some downsides of this prediction task could be that they would devote more resources to enforcing fee payment from one group of vendors more than another which may result in unbalanced enforcement or some groups thinking they can get away with not paying the fee more easily. Similarly, there may be a better way of collecting payment and having a prediction method like this may prevent the government from spending time/money to find a better way of collecting or setting up the fee. In my opinion, I don't think this kind of prediction is mostly a good idea. I don't know more than the homework guide gave us, but to help improve markets this fee seems important and having ways to make fee payment more common will help that. 



